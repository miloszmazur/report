The idea of evolving Behavior Trees using Genetic Programming method is not an entirely new concept. Similarity in representation of the specimen (both Behavior Trees and originally proposed by J. R. Koza in \cite{kozagp} lisp programs are represented as a tree structure) allowed for nigh painless adaptation of the GP algorithm to the problem, while the popularity of Behavior Trees as the AI representation of choice has grown past game programming areas exclusively, venturing into previously unexplored use cases.

Earlier implementations of this idea include evolving Behavior Tree agents for a turn strategy game DEFCON in \cite{defconbt}. In this paper, authors applied Genetic Programming method to evolve Behavior Trees acting as an artificial agent playing the game against existing AI developed by Introversion Software Ltd. (creators of the game). The goal was producing a automated agent able to beat the reference bot more than 50\% of time over a sufficiently large number of games. Their experiments resulted with an evolved AI-bot able to beat the reference agent 55\% of the time in span of 200 games. The chosen method involved manually defining a Beavior Tree, then evolutionary swapping and modifying its branches and node parameters. The finished agent was the sum of partial Behavior Trees (utilizing the modularity feature) that were evolved for a specific behavior, a sub-goal, like resource positoning or fleet production.

Another research applied the method to evolve a controller for DelFly Explorer UAV, tasked with locating and flying through a window placed in a simulated (and later on, physical) room \cite{ksheperthesis}. In his thesis, Scheper constructed and used a framework of constructing and evolving Behavior Trees to obtain an agent with 88\% success rate in a simulated environment and 54\% in a real one. Furthermore, Scheper was able to reencounter and better describe some of the \cite{defconbt} findings related to evolving Behavior Trees specifically, like the low number of generations needed achieve a fit solution, indicated by mean fitness reaching plateau. This was also the first time that evolutionary Behavior Trees were introduced into Robotics area of research.

Cooperation among artificial agents in multi-agent system environments is a well-known subject, with multiple papers detailing state-of-the-art techniques \cite{cooperationstateoftheart}, \cite{cooperativeandcompetitivelearning}. The agents in multi-agent systems can present a wide range of behaviors, depending on the environment: their behavior can be classified as either cooperative, competitive, or (in case when said behavior is not apparent enough to be classified) indistinguishable. \cite{cooperativeandcompetitivelearning} classifies agents as \textit{selfish} (maximizing their private utility function) or \textit{utilitarian} (maximizing the group utility), in order to focus on their intention rather than actual behaviors presented. Selfish agents can still present behaviors that could be classified as cooperative, as long as it is a viable option, especially in trivial or no-conflict games (per \cite{cooperativeandcompetitivelearning} classification).

A classic example of a matrix game with cooperative behaviors possible, and a model subject for many behavior-related studies (among them, referenced in this work \cite{cooperativeandcompetitivelearning}, \cite{cooperationstateoftheart}, \cite{cooperationevolution}, \cite{evolutionofcooperationmechanisms}) is Prisoner's Dilemma. The game, though simple in nature, posed a serious problem to the existing game theory - the most rational action (to deflect) wouldn't neccesarily bring the highest payoff. \cite{cooperationevolution} formalized an iterated version of the game with unspecified number of interactions between the two prisoners to present how cooperation based on reciprocity, when introduced to the population, might evolve and finally establish itself, resisting invasion from other strategies in the population. Study by van den Berg \& Weissing \cite{evolutionofcooperationmechanisms}, however, discovered and related how changes in evolutionary strategy and mutation regime can bring a vastly different in evolutionary outcome (average level of cooperation, specific cooperation strategy). The methods used in the study were 1:1 genotype-phenotype mapping and a simple neural network.

Finally, there seems to be a constant, unsatiated need for capable testing environments. Undoubtedly, modifying and fine-tuning features and code of the testing environment are the most sought-after features, this being the reason why so many researhers turn to in-house developed solutions. There were, however, a number of attempts of introducing a general-purpose testing environment (\cite{mason}, \cite{aisandbox}, \cite{frailweb}) or, at the very least, \textit{generalized} testing environment. Still, openess of the code and ease of modifications seem to prevail being the deciding factors. % mention familiarity with the environment?
